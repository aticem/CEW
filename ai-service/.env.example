# ============================================================================
# CEW AI Service - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env (Linux/Mac)
#   Copy-Item .env.example .env (PowerShell)
# ============================================================================

# ----------------------------------------------------------------------------
# REQUIRED: OpenAI API Key
# ----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
# This is required for embeddings and chat completions
OPENAI_API_KEY=

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------
# Port the Express server will listen on
PORT=3001

# Environment mode (development | production)
NODE_ENV=development

# ----------------------------------------------------------------------------
# File Paths
# ----------------------------------------------------------------------------
# Directory where source documents are stored for ingestion
# Relative to ai-service/ or absolute path
DOCUMENTS_PATH=./documents

# Directory where vector index and document registry are persisted
# This folder is created automatically if it doesn't exist
INDEX_STORE_PATH=./index-store

# Optional: Path to CEW frontend public folder (for QAQC documents)
# CEW_QAQC_PATH=../CEW1/_root/public/QAQC

# ----------------------------------------------------------------------------
# LLM (Language Model) Settings
# ----------------------------------------------------------------------------
# OpenAI model for chat completions
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# gpt-4o-mini is recommended for cost-effective performance
LLM_MODEL=gpt-4o-mini

# Temperature controls randomness (0.0 = deterministic, 1.0 = creative)
# Lower values recommended for factual Q&A
LLM_TEMPERATURE=0.1

# Maximum tokens in the response
# Higher values allow longer answers but cost more
LLM_MAX_TOKENS=1000

# ----------------------------------------------------------------------------
# Embedding Model Settings
# ----------------------------------------------------------------------------
# OpenAI model for generating text embeddings
# text-embedding-3-small is cost-effective with good quality
# text-embedding-3-large is higher quality but more expensive
EMBEDDING_MODEL=text-embedding-3-small

# ----------------------------------------------------------------------------
# Document Chunking Settings
# ----------------------------------------------------------------------------
# Maximum size of each text chunk in characters
# Smaller chunks = more precise retrieval, larger = more context
CHUNK_SIZE=1000

# Overlap between consecutive chunks in characters
# Helps preserve context across chunk boundaries
CHUNK_OVERLAP=200

# ----------------------------------------------------------------------------
# Retrieval Settings
# ----------------------------------------------------------------------------
# Minimum similarity score (0.0 - 1.0) for retrieved chunks
# Higher = more relevant but fewer results
SCORE_THRESHOLD=0.7

# Maximum number of chunks to retrieve per query
TOP_K=5

# ----------------------------------------------------------------------------
# Logging
# ----------------------------------------------------------------------------
# Log level: error, warn, info, debug
LOG_LEVEL=info

# Optional: Log file path (leave empty for console only)
# LOG_FILE=./logs/ai-service.log

# ----------------------------------------------------------------------------
# OCR Settings (Optional)
# ----------------------------------------------------------------------------
# Enable OCR for scanned PDFs
OCR_ENABLED=false

# Tesseract languages (comma-separated)
# eng = English, tur = Turkish
TESSERACT_LANG=eng,tur

# ----------------------------------------------------------------------------
# Vector Store Settings (Optional)
# ----------------------------------------------------------------------------
# ChromaDB host (if using external ChromaDB server)
# CHROMA_HOST=localhost
# CHROMA_PORT=8000
